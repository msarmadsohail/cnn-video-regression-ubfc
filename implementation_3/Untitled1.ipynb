{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["WlYLzShHPOHO","WPhuwnXOPZwG"],"mount_file_id":"1v81KNxpbo-BjYW7UfagSP9NRobT13ZrQ","authorship_tag":"ABX9TyNQEYtMTFzwLvE1RCHtqoRm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Importing Dataset"],"metadata":{"id":"WlYLzShHPOHO"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"cTSzMuprO5x8","executionInfo":{"status":"ok","timestamp":1692691135473,"user_tz":-300,"elapsed":7265,"user":{"displayName":"Sarmad Sohail","userId":"11416326453917330793"}}},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","import matplotlib.pyplot as plt\n","from google.colab.patches import cv2_imshow\n","\n","\n","class HeartRateDataset(Dataset):\n","    def __init__(self, data_path, frame_length, pixel_size):\n","        self.data_path = data_path\n","        self.frame_length = frame_length\n","        self.pixel_size = pixel_size\n","        self.video_paths, self.labels = self._read_data()\n","\n","    def __len__(self):\n","        return len(self.video_paths)\n","\n","    def __getitem__(self, index):\n","        self.index = index\n","        video_path = self.video_paths[index]\n","        label = self.labels[index]\n","        # print(video_path, index, label)\n","        clips, clips_labels = self._load_video(video_path, label)\n","        return clips, clips_labels\n","\n","    def _read_data(self):\n","        video_paths = []\n","        labels = []\n","        # Excluding Some Videos for testing Purposes\n","        # excluded_directory = []\n","        excluded_directory = [              # subject1, 2, 3 & 4 shouldn't be in this list because of them all others are excluded too\n","        \"/content/drive/MyDrive/UBFC_DATASET/DATASET_2/subject8\",\n","        \"/content/drive/MyDrive/UBFC_DATASET/DATASET_2/subject9\",\n","        \"/content/drive/MyDrive/UBFC_DATASET/DATASET_2/subject17\",\n","        \"/content/drive/MyDrive/UBFC_DATASET/DATASET_2/subject23\",\n","        \"/content/drive/MyDrive/UBFC_DATASET/DATASET_2/subject27\",\n","        \"/content/drive/MyDrive/UBFC_DATASET/DATASET_2/subject31\"\n","    ]\n","# Skipping /content/drive/MyDrive/UBFC_DATASET/DATASET_2/subject17/vid.avi due to being in an excluded directory\n","# Skipping /content/drive/MyDrive/UBFC_DATASET/DATASET_2/subject23/vid.avi due to being in an excluded directory\n","# Skipping /content/drive/MyDrive/UBFC_DATASET/DATASET_2/subject31/vid.avi due to being in an excluded directory\n","# Skipping /content/drive/MyDrive/UBFC_DATASET/DATASET_2/subject8/vid.avi due to being in an excluded directory\n","# Skipping /content/drive/MyDrive/UBFC_DATASET/DATASET_2/subject9/vid.avi due to being in an excluded directory\n","\n","        for root, _, files in os.walk(self.data_path):\n","          if \"vid.avi\" in files and \"ground_truth.txt\" in files:\n","              video_path = os.path.join(root, \"vid.avi\")\n","              ground_truth_file = os.path.join(root, \"ground_truth.txt\")\n","\n","              # Check if the current root directory should be excluded\n","              if any(excluded_dir in root for excluded_dir in excluded_directory):\n","                # video_path = os.path.join(root, \"vid.avi\")\n","                # ground_truth_file = os.path.join(root, \"ground_truth.txt\")\n","                print(f\"Skipping {video_path} due to being in an excluded directory\")\n","                continue\n","              else:\n","                with open(ground_truth_file, 'r') as f:\n","                    # Read the first two lines\n","                    lines = f.readlines()\n","                    if len(lines) >= 2:\n","                        # Split the second line into numerical values\n","                        label_line = lines[1].strip()\n","                        label_values = [float(val) for val in label_line.split()]\n","                        video_paths.append(video_path)\n","                        labels.append(label_values)\n","                    else:\n","                        print(f\"Skipping {video_path} due to missing second line in ground_truth.txt\")\n","        return video_paths, labels\n","\n","\n","    def _load_video(self, video_path, label):\n","        cap = cv2.VideoCapture(video_path)\n","        video_frames = []\n","        frame_indices = []\n","        frame_count = 0\n","        while True:\n","            ret, frame = cap.read()\n","            # plt.subplot(3,3,ret);plt.imshow(frame)\n","            if not ret:\n","                break\n","            video_frames.append(frame)\n","            frame_indices.append(frame_count)\n","            frame_count += 1\n","            # plt.subplot(50,50,frame_count);plt.imshow(frame)\n","        cap.release()\n","\n","        # print(frame_indices)\n","        # Randomly choose 16 frame indices\n","\n","        if len(frame_indices) < self.frame_length:\n","          print(\"Error: Not enough frames for sampling.\")\n","          return 0, 0  # Return None to indicate that no clips were generated\n","\n","        selected_indices = np.random.choice(frame_indices, size=self.frame_length, replace=False)\n","        # Convert labels to a NumPy array\n","        labels_array = np.array(label)\n","        # Use the selected indices to retrieve corresponding heart rate labels\n","        selected_labels = labels_array[selected_indices]  # Select the appropriate columns\n","        selected_frames = [video_frames[i] for i in selected_indices]\n","        selected_frames = self._preprocess_frames(selected_frames)\n","        video_tensor = torch.tensor(selected_frames)  # Convert NumPy array to PyTorch tensor\n","        # self._imshow_frames(selected_frames)\n","\n","        if len(video_tensor) < self.frame_length:\n","          num_repeats = self.frame_length // len(video_tensor) + 1\n","          video_tensor = video_tensor.repeat(num_repeats, 1, 1, 1)\n","\n","        # Create clips with the selected frames and corresponding labels\n","        clips = []\n","        clips_labels = []\n","        for i in range(len(selected_frames) - self.frame_length + 1):\n","            clip = selected_frames[i:i + self.frame_length]\n","            clip_labels = selected_labels[i:i + self.frame_length]\n","            # clips.append(clip)\n","            clips.append(clip)  # Include the selected labels for each clip\n","            clips_labels.append(clip_labels)\n","\n","        self._imshow_clips(clips, clips_labels)\n","        return clips, clips_labels\n","\n","    def _preprocess_frames(self, video_frames):\n","        # Resize and normalize frames\n","        processed_frames = [cv2.resize(frame, (self.pixel_size, self.pixel_size)) for frame in video_frames]\n","        processed_frames = np.array(processed_frames, dtype=np.float32)\n","        processed_frames /= 255.0  # Normalize to [0, 1]\n","        return processed_frames\n","\n","    def _imshow_frames(self, video_frames):\n","        # Resize and normalize frames\n","        selected_frames = video_frames * 255.0\n","        # Save the frames to a directory\n","        output_dir = \"/content/SAVE_FRAMES/View_Frames\"\n","        os.makedirs(output_dir, exist_ok=True)\n","        for idx, frame in enumerate(selected_frames):\n","            frame_filename = os.path.join(output_dir, f\"frame_{idx}.png\")\n","            plt.imshow(frame)\n","            cv2.imwrite(frame_filename, frame)\n","        # Visualize frames outside the loop\n","        num_visualize = min(3, len(selected_frames))  # Ensure not to exceed available frames\n","        visualize_filenames = np.random.choice(os.listdir(output_dir), size=num_visualize, replace=False)\n","        print(\"Filenames in output directory:\", visualize_filenames)\n","        for filename in visualize_filenames:\n","            frame_path = os.path.join(output_dir, filename)\n","            frame = cv2.imread(frame_path)\n","            plt.imshow(frame)\n","            plt.title(f\"Selected Frame: {filename}\")\n","            plt.show()\n","\n","    def _imshow_clips(self, clips, labels):\n","      output_dir = \"/content/SAVE_FRAMES/View_Clips\"\n","      os.makedirs(output_dir, exist_ok=True)\n","\n","      for idx, (clip, label) in enumerate(zip(clips, labels)):\n","          clip_frames = [frame * 255.0 for frame in clip]  # Multiply each frame by 255.0\n","          clip_filename = os.path.join(output_dir, f\"Clip_{self.index}_{idx}.avi\")  # Include self.index in the filename\n","\n","          clip_height, clip_width = clip_frames[0].shape[:2]\n","          fourcc = cv2.VideoWriter_fourcc(*'XVID')\n","          clip_writer = cv2.VideoWriter(clip_filename, fourcc, 30.0, (clip_width, clip_height))\n","          for frame in clip_frames:\n","              frame = frame.astype(np.uint8)\n","              clip_writer.write(frame)\n","          clip_writer.release()\n","      # Rest of the visualization code\n","      num_visualize = min(3, len(clips))  # Visualize up to 3 clips\n","      visualize_indices = np.random.choice(len(clips), size=num_visualize, replace=False)\n","      for idx in visualize_indices:\n","          clip = clips[idx]\n","          label = labels[idx]\n","          plt.figure(figsize=(12, 4))\n","          for i, frame in enumerate(clip):\n","              plt.subplot(1, len(clip), i + 1)\n","              plt.imshow(frame)\n","              plt.title(i + 1)\n","              plt.axis('off')\n","          plt.suptitle(f\"Selected Clip with Label: {label}\", fontsize=14)\n","          plt.show()"]},{"cell_type":"code","source":["# Define the dataset and data loader\n","data_path = '/content/drive/MyDrive/UBFC_DATASET/DATASET_2'\n","frame_length = 5\n","pixel_size = 224\n","batch_size = 12\n","dataset = HeartRateDataset(data_path, frame_length, pixel_size)\n","data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b982ykviPWtu","executionInfo":{"status":"ok","timestamp":1692691147873,"user_tz":-300,"elapsed":12403,"user":{"displayName":"Sarmad Sohail","userId":"11416326453917330793"}},"outputId":"fe932308-6949-45ed-ee11-7ed0636e44b0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Skipping /content/drive/MyDrive/UBFC_DATASET/DATASET_2/subject17/vid.avi due to being in an excluded directory\n","Skipping /content/drive/MyDrive/UBFC_DATASET/DATASET_2/subject23/vid.avi due to being in an excluded directory\n","Skipping /content/drive/MyDrive/UBFC_DATASET/DATASET_2/subject27/vid.avi due to being in an excluded directory\n","Skipping /content/drive/MyDrive/UBFC_DATASET/DATASET_2/subject31/vid.avi due to being in an excluded directory\n","Skipping /content/drive/MyDrive/UBFC_DATASET/DATASET_2/subject8/vid.avi due to being in an excluded directory\n","Skipping /content/drive/MyDrive/UBFC_DATASET/DATASET_2/subject9/vid.avi due to being in an excluded directory\n"]}]},{"cell_type":"markdown","source":["## ResNet18 Model_1_up_1"],"metadata":{"id":"WPhuwnXOPZwG"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torchvision import models\n","\n","import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","\n","class SelfAttention(nn.Module):\n","    def __init__(self, in_channels):\n","        super(SelfAttention, self).__init__()\n","        self.query = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n","        self.key = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n","        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n","        self.gamma = nn.Parameter(torch.zeros(1))\n","\n","    def forward(self, x):\n","        batch_size, channels, height, width = x.size()\n","\n","        query = self.query(x).view(batch_size, -1, width * height).permute(0, 2, 1)  # B, HW, C'\n","        key = self.key(x).view(batch_size, -1, width * height)  # B, C', HW\n","        value = self.value(x).view(batch_size, -1, width * height)  # B, C, HW\n","\n","        attention = torch.matmul(query, key)  # B, HW, HW\n","        attention = nn.functional.softmax(attention, dim=1)\n","\n","        weighted = torch.matmul(value, attention).view(batch_size, channels, height, width)\n","\n","        out = self.gamma * weighted + x\n","        return out\n","\n","\n","class ResNet18_HeartRate(nn.Module):\n","    def __init__(self, num_classes, frame_length, dropout_prob=0.5):\n","        super(ResNet18_HeartRate, self).__init__()\n","        self.frame_length = frame_length\n","        self.num_classes = num_classes\n","        self.dropout_prob = dropout_prob\n","\n","        resnet18 = models.resnet18(pretrained=False)\n","        resnet18.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","\n","        self.resnet18_features = nn.Sequential(*list(resnet18.children())[:-2])\n","        self.batch_norm = nn.BatchNorm2d(512)\n","        self.temporal_pooling = nn.AdaptiveAvgPool2d((1, 1))\n","        self.attention = SelfAttention(512)\n","        self.fc1 = nn.Linear(512, 256)\n","        self.fc2 = nn.Linear(256, frame_length)\n","        self.dropout = nn.Dropout(self.dropout_prob)\n","\n","    def forward(self, x):\n","        # print('A', x.shape)\n","        num_clips, batch_size, frames, height, width, channels = x.size()\n","\n","        # Reshape input to (batch_size * num_clips * frames, channels, height, width)\n","        x = x.view(-1, channels, height, width)\n","        # print('AA', x.shape)\n","\n","        features = self.resnet18_features(x)\n","        features = self.batch_norm(features)\n","        features = self.attention(features)\n","\n","\n","        pooled_features = self.temporal_pooling(features).view(batch_size * num_clips, frames, -1)\n","\n","        x = self.fc1(pooled_features)\n","        x = self.dropout(x)\n","        intermediate_predictions = self.fc2(x)  # Intermediate predictions with shape [batch_size * num_clips, frames, 32]\n","\n","        # Introduce an additional fully connected layer to map [32, 32] to [32]\n","        intermediate_predictions = intermediate_predictions.mean(dim=1)\n","        predictions = intermediate_predictions.view(num_clips, batch_size, frames)\n","        # print('AAAA', predictions.shape)\n","\n","        return predictions\n","\n","\n","\n","# Example usage:\n","num_classes = 1\n","num_clips = 1\n","\n","# Create an instance of the ResNet18_HeartRate model\n","model = ResNet18_HeartRate(num_classes, frame_length)\n","\n","# Sample input tensor with shape (num_clips, batch_size, frames, channels, height, width)\n","sample_input = torch.randn(num_clips, batch_size, frame_length, 224, 224, 3)\n","\n","# Forward pass to get heart rate predictions\n","predictions = model(sample_input)\n","\n","# The 'predictions' tensor will have shape (batch_size, frames)\n","print(predictions.shape)\n","print(predictions)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hn4nQmziPfuz","executionInfo":{"status":"ok","timestamp":1692691154207,"user_tz":-300,"elapsed":6339,"user":{"displayName":"Sarmad Sohail","userId":"11416326453917330793"}},"outputId":"2c2a1734-f004-4840-8283-54b9164cce74"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([1, 12, 5])\n","tensor([[[-0.0138,  0.0351, -0.0030, -0.0254,  0.0238],\n","         [ 0.0070,  0.0097, -0.0172, -0.0114,  0.1115],\n","         [-0.0015,  0.0895,  0.0033, -0.0167,  0.0605],\n","         [-0.0608,  0.0491,  0.0084, -0.0021,  0.0700],\n","         [ 0.0246,  0.1206, -0.0765, -0.0359,  0.0674],\n","         [-0.0428,  0.0589, -0.0023, -0.0315,  0.0455],\n","         [ 0.0112,  0.0763,  0.0340, -0.0178,  0.0217],\n","         [ 0.0105,  0.0828, -0.0444,  0.0169,  0.0659],\n","         [-0.0543,  0.0718, -0.0127, -0.0125,  0.0719],\n","         [-0.0364,  0.0967, -0.0505, -0.0430,  0.0884],\n","         [-0.0392,  0.0729, -0.0792,  0.0697,  0.0650],\n","         [ 0.0308,  0.0652, -0.0571,  0.0205,  0.0779]]],\n","       grad_fn=<ViewBackward0>)\n"]}]},{"cell_type":"markdown","source":["## Retraing Model_1_up"],"metadata":{"id":"2i6jrh3LPnKL"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","# Load the saved model's state dictionary\n","model_state_dict = torch.load('/content/drive/MyDrive/1 Google Colab/CNN Internship/Implementation 3 17-08-2023/model_1_up.pth')\n","\n","# Define the new frame length\n","frame_length_new = 5  # Adjust this according to your needs\n","\n","# Get the number of classes from the saved state dictionary\n","num_classes = model_state_dict['fc2.bias'].shape[0]\n","# print(num_classes)\n","\n","# Create a new instance of the ResNet18_HeartRate model with the new frame length\n","model_new = ResNet18_HeartRate(num_classes, frame_length_new)\n","\n","# Load the weights and biases from the saved model's state dictionary\n","# Adjust the keys in the state dictionary to match the new model's architecture\n","new_state_dict = {}\n","# print(np.array(model_state_dict))\n","for key, value in model_state_dict.items():\n","    if 'fc2' in key:\n","        # Replace the last fully connected layer's weights and biases\n","        new_state_dict[key] = model_new.state_dict()[key]\n","    else:\n","        new_state_dict[key] = value\n","# print(new_state_dict)\n","# Load the modified state dictionary into the new model\n","model_new.load_state_dict(new_state_dict)\n","\n","# Set the new model in evaluation mode\n","model_new.eval()\n","\n","# Now you can use the model_new with the adjusted frame length\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YN3DUkRgPrhg","executionInfo":{"status":"ok","timestamp":1692691159320,"user_tz":-300,"elapsed":5117,"user":{"displayName":"Sarmad Sohail","userId":"11416326453917330793"}},"outputId":"31e60906-93aa-4e21-d3a6-9e20ef6d39c6"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet18_HeartRate(\n","  (resnet18_features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (4): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (5): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (6): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (7): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","  )\n","  (batch_norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (temporal_pooling): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (attention): SelfAttention(\n","    (query): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n","    (key): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n","    (value): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n","  )\n","  (fc1): Linear(in_features=512, out_features=256, bias=True)\n","  (fc2): Linear(in_features=256, out_features=5, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# Define Mean Squared Error (MSE) loss function and optimizer\n","loss_fn = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","num_classes = 1\n","num_clips = 1\n","\n","# Training and validation loop\n","num_epochs = 5  # Example number of epochs, adjust as needed\n","validation_frequency = 5  # Perform validation every 3 epochs\n","best_validation_loss = float('inf')  # Initialize with a large value\n","\n","batch_losses_per_epoch = []\n","batch_loss = []\n","validation_losses = []\n","predicted_values = []\n","true_values = []\n","\n","for epoch in range(num_epochs):\n","    for batch_index, (video_clips, batch_labels) in enumerate(data_loader):\n","      if video_clips != 0 and batch_labels != 0:\n","        # Convert the lists to tensors\n","        video_clips = torch.stack(video_clips)\n","        # print(video_clips.shape, video_clips)\n","        batch_labels = torch.stack(batch_labels)\n","        # print(batch_labels.shape, batch_labels)\n","\n","        optimizer.zero_grad()\n","\n","        # Forward pass to get heart rate predictions\n","        predictions = model(video_clips)\n","        predicted_values.append((epoch, batch_index, predictions))\n","        true_values.append((epoch, batch_index, batch_labels))\n","\n","\n","        # Convert batch_labels to a tensor with appropriate dimensions and data type\n","        batch_labels_tensor = torch.tensor(batch_labels, dtype=torch.float32)\n","        print(\"Predictions Vector Shape: \", predictions.shape)\n","        print(\"Batch Labels Shape: \", batch_labels.shape)\n","\n","        # Compute the loss using Mean Squared Error (MSE) loss\n","        loss = loss_fn(predictions, batch_labels_tensor)\n","\n","        if model.training:\n","            # Backward pass and update model parameters only during training\n","            loss.backward()\n","            optimizer.step()\n","\n","            batch_loss.append((epoch, batch_index, loss.item()))  # Store batch loss with epoch and batch index\n","            batch_losses_per_epoch.append((epoch, loss.item()))  # Store batch loss for the current epoch\n","\n","            # Print the loss at the end of each batch\n","            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch Loss: {loss.item()}\")\n","\n","        # Perform validation step every 'validation_frequency' epochs\n","        if (epoch + 1) % validation_frequency == 0:\n","            model.eval()  # Switch to evaluation mode\n","            with torch.no_grad():\n","                validation_loss = 0.0\n","                num_batches = 0\n","\n","                try:\n","                  for val_video_clips, val_batch_labels in data_loader:  # Use validation data_loader\n","                    if video_clips is not None and batch_labels is not None:\n","                      val_video_clips = torch.stack(val_video_clips)\n","                      val_batch_labels = torch.stack(val_batch_labels)\n","\n","                      # Forward pass to get validation predictions\n","                      val_predictions = model(val_video_clips)\n","\n","                      # Convert validation batch_labels to a tensor with appropriate dimensions and data type\n","                      val_batch_labels_tensor = torch.tensor(val_batch_labels, dtype=torch.float32)\n","\n","                      # Compute validation loss using Mean Squared Error (MSE) loss\n","                      val_loss = loss_fn(val_predictions, val_batch_labels_tensor)\n","\n","                      # Accumulate validation loss\n","                      validation_loss += val_loss.item()\n","                      num_batches += 1\n","                    else:\n","                      print(\"Not enough frames for sampling. Skipping this batch...\")\n","\n","                except ValueError as e:\n","                    print(\"Error:\", e)\n","                    continue\n","\n","                # Calculate average validation loss\n","                average_validation_loss = validation_loss / num_batches\n","                validation_losses.append((epoch+1, average_validation_loss))\n","\n","                # Print validation loss\n","                print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {average_validation_loss}\")\n","\n","                # Update best validation loss and save model checkpoint if validation loss improves\n","                if average_validation_loss < best_validation_loss:\n","                    best_validation_loss = average_validation_loss\n","                    torch.save(model.state_dict(), '/content/drive/MyDrive/1 Google Colab/CNN Internship/Implementation 3 17-08-2023/model_1_up_1.pth')\n","\n","            model.train()  # Switch back to training model\n","      else:\n","        print(\"Not enough frames for sampling. Skipping this batch...\")\n","\n","\n","# Training completed\n","torch.save(model.state_dict(), '/content/drive/MyDrive/1 Google Colab/CNN Internship/Implementation 3 17-08-2023/model_1_up_1.pth')"],"metadata":{"id":"0WgGWv5VPvW6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Results"],"metadata":{"id":"AYjN4M5lP7WL"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# print(batch_losses_per_epoch, '\\n')\n","# print(batch_loss, '\\n')\n","# print(validation_losses, '\\n')\n","\n","# Calculate batch indices for batch_losses_per_epoch\n","epoch_batch_indices = [f'Epoch: {epoch+1}'  for epoch in range(num_epochs)]\n","epoch_batch_losses = [sum(loss for epoch_val, loss in batch_losses_per_epoch if epoch_val == epoch) for epoch in range(num_epochs)]\n","# Plot epoch_batch_loss\n","fig, ax = plt.subplots(figsize=(10, 5))\n","ax.plot(epoch_batch_indices, epoch_batch_losses, marker='o', linestyle='-', color='blue', label='Epoch Batch Loss')\n","ax.set_xlabel('Loss Data Index')\n","ax.set_ylabel('Loss')\n","ax.set_title('Epoch Batch Loss Curve')\n","ax.set_xticklabels(epoch_batch_indices, rotation=45, ha='right')\n","# ax.set_ylim(min(epoch_batch_losses) - 10, max(epoch_batch_losses) + 10)\n","plt.legend()\n","plt.show()\n","\n","\n","# Calculate batch indices for batch_loss\n","batch_indices = [f'Batch: {batch_index+1} \\nEpoch: {epoch+1}' for epoch, batch_index, _ in batch_loss]\n","batch_losses = [loss for _ , _ , loss in batch_loss]\n","# Plot batch_loss\n","fig, ax = plt.subplots(figsize=(10, 5))\n","ax.plot(batch_indices, batch_losses, marker='s', linestyle='-', color='green', label='Batch Loss')\n","ax.set_xlabel('Batches (within Epochs)')\n","ax.set_ylabel('Loss')\n","ax.set_title('Batch Loss Curve')\n","ax.set_xticklabels(batch_indices, rotation=45, ha='right')\n","# ax.set_ylim(min(batch_losses) - 10, max(batch_losses) + 10)\n","plt.legend()\n","plt.show()\n","\n","\n","# Calculate validation indices for validation_losses\n","def val_loss_calc(validation_losses):\n","    loss_groups = {}  # Dictionary to store grouped losses\n","    for x, loss in validation_losses:\n","        if x not in loss_groups:\n","            loss_groups[x] = []\n","        loss_groups[x].append(loss)\n","    grouped_losses = [losses for x, losses in loss_groups.items()]\n","    x_values = list(loss_groups.keys())\n","    return grouped_losses, x_values\n","validation_loss, validation_indices = val_loss_calc(validation_losses)\n","\n","# Plot validation_losses\n","fig, ax = plt.subplots(figsize=(10, 5))\n","bar_colors = ['blue', 'green', 'orange', 'red', 'purple', 'cyan', 'magenta', 'yellow']\n","num_entries = len(validation_indices)\n","x_ticks = np.arange(num_entries)  # Create ticks for x-axis\n","bar_width = 0.1  # Width of each bar\n","\n","# Plot the bars for each entry\n","for i, y_values in enumerate(validation_loss):\n","    x_positions = x_ticks[i] + np.linspace(-bar_width/2, bar_width/2, len(y_values))\n","    ax.bar(x_positions, y_values, bar_width, label=f'Entry {i+1}', color=bar_colors[i::num_entries])\n","\n","ax.set_xticks(x_ticks)\n","ax.set_xticklabels(validation_indices)\n","ax.set_xlabel('Epoch_no')\n","ax.set_ylabel('Loss')\n","ax.set_title('Validation Loss Bar Graph')\n","ax.set_xticklabels(validation_indices, rotation=45, ha='left')\n","# # ax.set_ylim(min(validation_loss) - 10, max(validation_loss) + 10)\n","plt.show()"],"metadata":{"id":"GZw0BxppQUE0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Access the predicted values and true values at the chosen index\n","chosen_predicted_values = [predictions.detach().numpy() for epoch, batch_index, predictions in predicted_values]\n","chosen_true_values = [batch_labels.detach().numpy() for epoch, batch_index, batch_labels in true_values]\n","chosen_indices = [f'Batch: {batch_index+1} \\nEpoch: {epoch+1}' for epoch, batch_index, _ in predicted_values]\n","chosen_predicted_values = np.array(chosen_predicted_values)\n","chosen_true_values = np.array(chosen_true_values)\n","chosen_indices = np.array(chosen_indices)\n","\n","# print(chosen_predicted_values)\n","# print(chosen_true_values)\n","# print(chosen_indices.shape, chosen_indices)\n","\n","# Plot Predicted vs True Values Curve\n","fig, ax = plt.subplots(figsize=(10, 5))\n","# for color_index in range(chosen_predicted_values.shape[2]):\n","#     ax.plot(chosen_predicted_values[:, :, color_index].flatten(), marker='o', linestyle='-', label=f'Predicted Color {color_index+1}')\n","#     ax.plot(chosen_true_values[:, :, color_index].flatten(), marker='s', linestyle='-', label=f'True Color {color_index+1}')\n","\n","ax.plot(chosen_predicted_values.flatten(), marker='o', linestyle='-', color = 'blue', label = 'Prediction', markersize=1, linewidth=0.05)\n","ax.plot(chosen_true_values.flatten(), marker='s', linestyle='-', color = 'green', label = 'True Value', markersize=1, linewidth=0.05)\n","ax.set_xlabel('Batches (within Epochs)')\n","ax.set_ylabel('Value')\n","ax.set_title('Predicted vs True Values Curve')\n","# ax.set_xticklabels(chosen_indices, rotation=45, ha='right')\n","# ax.set_ylim(min(epoch_batch_losses) - 10, max(epoch_batch_losses) + 10)\n","plt.grid()\n","plt.legend()\n","plt.show()\n","\n","\n","# Plot Predicted vs True Values Slope\n","common_axis_min = -10\n","common_axis_max = max(max(chosen_predicted_values.flatten()), max(chosen_true_values.flatten())) + 10\n","x = np.linspace(common_axis_min, common_axis_max)\n","fig, ax = plt.subplots(figsize=(10, 5))\n","ax.scatter(chosen_true_values.flatten(), chosen_predicted_values.flatten(), marker='o', linestyle='-', s = 1, label = 'Predictions')\n","ax.scatter(x, x, marker='s', linestyle='solid', color = 'green', s = 0.9, label = 'Ideal Model')\n","ax.set_xlabel('True Values')\n","ax.set_ylabel('Predictions')\n","ax.set_title('Predicted vs True Values Slope')\n","# plt.xlim(common_axis_min, common_axis_max)\n","# plt.ylim(common_axis_min, common_axis_max)\n","plt.grid()\n","plt.legend()\n","plt.show()"],"metadata":{"id":"tAJ2uoo3QS30"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Testing Model_1_up_1\n","Comments: More Horrible Prediction\n","\n","Transfer Learning Implemented to test over updated frame_size"],"metadata":{"id":"EGDp4FpLQeTr"}},{"cell_type":"code","source":["excluded_directory_testSet = [\n","        \"/content/drive/MyDrive/UBFC_DATASET/DATASET_2/subject8\",\n","        \"/content/drive/MyDrive/UBFC_DATASET/DATASET_2/subject9\",\n","        \"/content/drive/MyDrive/UBFC_DATASET/DATASET_2/subject17\",\n","        \"/content/drive/MyDrive/UBFC_DATASET/DATASET_2/subject23\",\n","        \"/content/drive/MyDrive/UBFC_DATASET/DATASET_2/subject27\",\n","        \"/content/drive/MyDrive/UBFC_DATASET/DATASET_2/subject31\"\n","    ]\n","\n","\n","frame_length_new = 25\n","excluded_directory = []\n","pixel_size = 224\n","batch_size = 36\n","num_classes = 1\n","num_clips = 1"],"metadata":{"id":"pGfHliwDQiGr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","# Load the saved model's state dictionary\n","model_state_dict = torch.load('/content/drive/MyDrive/1 Google Colab/CNN Internship/Implementation 3 17-08-2023/model_1_up_1.pth')\n","\n","# Define the new frame length\n","frame_length_new = 5  # Adjust this according to your needs\n","\n","# Get the number of classes from the saved state dictionary\n","num_classes = model_state_dict['fc2.bias'].shape[0]\n","# print(num_classes)\n","\n","# Create a new instance of the ResNet18_HeartRate model with the new frame length\n","model_new = ResNet18_HeartRate(num_classes, frame_length_new)\n","\n","# Load the weights and biases from the saved model's state dictionary\n","# Adjust the keys in the state dictionary to match the new model's architecture\n","new_state_dict = {}\n","# print(np.array(model_state_dict))\n","for key, value in model_state_dict.items():\n","    if 'fc2' in key:\n","        # Replace the last fully connected layer's weights and biases\n","        new_state_dict[key] = model_new.state_dict()[key]\n","    else:\n","        new_state_dict[key] = value\n","# print(new_state_dict)\n","# Load the modified state dictionary into the new model\n","model_new.load_state_dict(new_state_dict)\n","\n","# Set the new model in evaluation mode\n","model_new.eval()\n","\n","# Now you can use the model_new with the adjusted frame length\n"],"metadata":{"id":"5JLxrBvlQkQ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","\n","\n","# # Load the saved model's state dictionary\n","# model_state_dict = torch.load('/content/drive/MyDrive/1 Google Colab/CNN Internship/Implementation 2 11-08-2023/model_1.pth')\n","# model_new = ResNet18_HeartRate(num_classes, frame_length_new)  # Assuming frame_length_new is the desired new frame length\n","# model_new.load_state_dict(model_state_dict)\n","# model_new.eval()  # Set the model in evaluation mode\n","\n","# Define Mean Squared Error (MSE) loss function\n","loss_fn = nn.MSELoss()\n","\n","# Define a list to store evaluation losses\n","evaluation_losses = []\n","\n","# Define lists to store predicted and true values for visualization\n","predicted_values = []\n","true_values = []\n","\n","# Loop through the list of directories for evaluation\n","for directory_path in excluded_directory_testSet:\n","    test_dataset = HeartRateDataset(directory_path, frame_length_new, pixel_size)\n","    test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","    with torch.no_grad():\n","        for video_clips, batch_labels in test_data_loader:\n","            if video_clips is not None and batch_labels is not None:\n","                video_clips = torch.stack(video_clips)\n","                batch_labels = torch.stack(batch_labels)\n","\n","                # Forward pass to get predictions\n","                predictions = model_new(video_clips)\n","\n","                # Convert batch_labels to a tensor with appropriate dimensions and data type\n","                batch_labels_tensor = torch.tensor(batch_labels, dtype=torch.float32)\n","\n","                # Compute the loss using Mean Squared Error (MSE) loss\n","                loss = loss_fn(predictions, batch_labels_tensor)\n","\n","                # Accumulate evaluation loss\n","                evaluation_losses.append(loss.item())\n","\n","                # Store predicted and true values for visualization\n","                predicted_values.extend(predictions.tolist())\n","                true_values.extend(batch_labels.tolist())\n","\n","# Calculate average evaluation loss\n","average_evaluation_loss = sum(evaluation_losses) / len(evaluation_losses)\n","\n","# Print the results\n","print(f\"Average Evaluation Loss: {average_evaluation_loss}\")\n","\n","# Visualize predictions vs. true values using a scatter plot\n","plt.figure(figsize=(8, 6))\n","plt.scatter(true_values, predicted_values, color='blue', marker='o', alpha=0.5)\n","plt.xlabel(\"True Values\")\n","plt.ylabel(\"Predicted Values\")\n","plt.title(\"Predictions vs. True Values\")\n","plt.grid(True)\n","plt.show()\n","\n","true_values_array = np.array(true_values)\n","predicted_values_array = np.array(predicted_values)\n","# Visualize predictions vs. true values using a scatter plot\n","plt.figure(figsize=(8, 6))\n","plt.plot(true_values_array.flatten(), color='blue', marker='s', alpha=0.5, label = 'Ground Truth')\n","plt.plot(predicted_values_array.flatten(), color='green', marker='o', alpha=0.5, label = 'Predictions')\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Predicted Values\")\n","plt.title(\"Predictions vs. True Values\")\n","plt.grid(True)\n","plt.legend()\n","plt.show()"],"metadata":{"id":"2i_MBzYhQqw1"},"execution_count":null,"outputs":[]}]}